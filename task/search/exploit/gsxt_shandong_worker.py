#!/usr/bin/env python
# -*- coding:utf-8 -*-
import os
import random
import re

from pyquery import PyQuery
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver import DesiredCapabilities

from base.gsxt_base_worker import GsxtBaseWorker
from common import util
from common.global_field import Model

'''
1. 验证码破解方式
2. 包含出资信息
3. 包含年报信息
4. 添加完成统计信息
5. 完成列表页名称提取
'''


class GsxtShanDongWorker(GsxtBaseWorker):
    def __init__(self, **kwargs):
        GsxtBaseWorker.__init__(self, **kwargs)

    # phantomjs抓取
    def web_phantomjs(self, url, headers, proxy):
        from datetime import datetime
        now_time = datetime.now().strftime("%Y%m%d%H%M%S")  # 生成当前的时间
        random_num = random.randint(0, 1000)  # 生成随机数n,其中0<=n<=100
        if random_num <= 100:
            random_num = str(0) + str(random_num)
        unique_num = str(now_time) + str(random_num)

        proxy_type = 'http'
        if 'socks5' in proxy:
            proxy = proxy[9:]
            proxy_type = 'socks5'
        elif 'http' in proxy:
            proxy = proxy[7:]
            proxy_type = 'http'

        proxy_auth = None
        proxy_list = proxy.split("@")
        if len(proxy_list) >= 2:
            proxy = proxy_list[1]
            proxy_auth = proxy_list[0]

        log_path = '{}/log/{}{}.txt'.format(self.base_path, self.province, unique_num)
        service_args = ['--proxy={}'.format(proxy),
                        '--proxy-type={}'.format(proxy_type),
                        '--load-images=no',
                        '--disk-cache=no',
                        '--ignore-ssl-errors=true',
                        '--cookies-file={}'.format(log_path)
                        ]
        # 如果有用户信息
        if proxy_auth is not None:
            service_args.append("--proxy-auth={}".format(proxy_auth))

        desired_capabilities = DesiredCapabilities.PHANTOMJS.copy()
        for key, value in headers.iteritems():
            desired_capabilities['phantomjs.page.customHeaders.{}'.format(key)] = value

        executable_path = '{}/bin/{}/phantomjs'.format(self.base_path, util.get_system_info())
        driver = webdriver.PhantomJS(executable_path=executable_path,
                                     desired_capabilities=desired_capabilities,
                                     service_args=service_args)
        driver.implicitly_wait(30)
        driver.set_page_load_timeout(30)
        driver.set_script_timeout(30)

        try:
            driver.get(url)
            return driver.page_source, driver.get_cookies()
            # print driver.get_cookies()
        except TimeoutException as e:
            self.log.error("phantomjs访问超时...")
            self.log.exception(e)
            driver.execute_script('window.stop()')
            return None, None
        finally:
            try:
                driver.quit()
            except Exception as e:
                self.log.error("关闭phantomjs失败，需要命令行关闭..")
                self.log.exception(e)
                util.run_cmd(
                    "ps -ef | grep -i phantomjs | grep -v grep | grep {} | awk '{print $2}' | xargs kill".format(
                        unique_num))
            finally:
                # 删除cookie文件
                if os.path.exists(log_path):
                    os.remove(log_path)

    def get_token(self, session):

        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4,ja;q=0.2',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Host': self.host,
            'Proxy-Authorization': 'Basic Ympoejpiamh6',
            'Proxy-Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36',
        }
        url = "http://{host}/".format(host=self.host)
        proxy = self.get_random_proxy()['http']
        text, cookies = self.web_phantomjs(url, headers, proxy)
        if text is None:
            self.log.warn("phantomjs 获取网页失败..")
            return None

        regex = ur"name=\"_csrf\" value=\"(.*?)\""
        search_list = re.search(regex, text)
        if search_list is not None:
            for cookie in cookies:
                session.cookies[cookie['name']] = cookie['value']

            return search_list.group(1)

        self.log.warn('没有搜索到任何 token信息..proxy = {}'.format(proxy))
        return None

    def get_search_list_html(self, keyword, session):
        url_list = []
        token = self.get_token(session)
        if token is None:
            return url_list, self.SEARCH_ERROR

        self.log.info("token 获取成功: {}".format(token))

        try:
            result_url = 'http://{host}/pub/query/'.format(host=self.host)
            session.headers['X-CSRF-TOKEN'] = token
            post_data = {
                'keyword': keyword,
                'isjyyc': 0,
                'isyzwf': 0
            }
            result_resp = self.task_request(session, session.post, result_url, data=post_data)
            if result_resp is None:
                self.log.error('搜索列表失败....')
                return url_list, self.SEARCH_ERROR

            result_json = util.json_loads(result_resp.text)
            if result_json is None:
                self.log.error('json转换失败...{text}'.format(text=result_resp.text))
                return url_list, self.SEARCH_ERROR

            results = result_json.get('results', None)
            if results is None:
                self.log.error('没有对应的results字段信息')
                return url_list, self.SEARCH_ERROR

            url_list = self.get_url_list(results)
            return url_list, self.SEARCH_SUCCESS if len(url_list) > 0 else self.SEARCH_NOTHING_FIND
        except Exception as e:
            self.log.exception(e)
        return url_list, self.SEARCH_ERROR

    # 工商公示, 企业公示
    def get_url_list(self, result_json):
        param_list = []
        for item in result_json:
            pri_pid = item.get('pripid', None)
            if pri_pid is None:
                self.log.warn('没有pripid 字段')
                continue
            ent_name = item.get('entname', None)
            if ent_name is None:
                self.log.warn('没有entname字段')
                continue
            jq = PyQuery(ent_name, parser='html')
            company = jq.text()
            if company is None or company == '':
                continue
            search_name = company.replace(' ', '')
            if search_name == '':
                continue
            param = {
                'pripid': pri_pid,
                'search_name': search_name,
            }
            seed_code = item.get('uniscid')
            if seed_code is not None:
                param['unified_social_credit_code'] = seed_code

            param_list.append(param)
        return param_list

    @staticmethod
    def get_company_name(text):

        json_data = util.json_loads(text)
        if json_data is None:
            return None

        jbxx = json_data.get('jbxx', None)
        if jbxx is None:
            return None

        ent_name = jbxx.get('entname', None)
        if ent_name is None or ent_name == '':
            ent_name = jbxx.get('traname', None)
            if ent_name is None or ent_name == '':
                return None

        return ent_name.strip()

    def get_annual_detail(self, session, text, data):
        json_data = util.json_loads(text)
        if json_data is None:
            return

        for item in json_data:
            ancheid = item.get('ancheid', None)
            ancheyear = item.get('ancheyear', None)
            if ancheid is None:
                continue
            if ancheyear is None:
                continue
            url = 'http://{host}/pub/annual/qy/detail/{ancheid}'.format(
                host=self.host, ancheid=ancheid)
            r = self.task_request(session, session.post, url=url)
            if r is None:
                self.append_model(data, Model.annual_info, url, '',
                                  year=ancheyear,
                                  status=self.STATUS_FAIL,
                                  classify=Model.type_detail)
                continue
            self.append_model(data, Model.annual_info, url, r.text,
                              year=ancheyear,
                              classify=Model.type_detail)

    # 出资信息详情
    def get_contributive_info_detail(self, session, data, page_text):
        json_data = util.json_loads(page_text)
        if json_data is None:
            return
        czxx = json_data.get('czxx', None)
        if czxx is None:
            return

        for item in czxx:
            recid = item.get('recid', None)
            if recid is None:
                continue
            url = 'http://{host}/pub/czxx/{recid}'.format(host=self.host, recid=recid)
            r = self.task_request(session, session.post, url=url)
            if r is None:
                self.append_model(data, Model.contributive_info, url, '',
                                  status=self.STATUS_FAIL,
                                  classify=Model.type_detail)
            else:
                self.append_model(data, Model.contributive_info, url, r.text,
                                  classify=Model.type_detail)

    def get_detail_html_list_qy(self, seed, search_name, session, pri_pid, flag):

        # 获得pri_pid
        pri_pid = self.get_pri_pid(session, pri_pid, flag)
        if pri_pid is None:
            self.log.warn('获取pripid失败: seed = {} search_name = {}'.format(seed, search_name))
            return None

        base_info_url = 'http://{host}/pub/jbxx/{flag}/{pripid}'.format(
            host=self.host, pripid=pri_pid, flag=flag)
        base_info = self.task_request(session, session.post, base_info_url)
        if base_info is None:
            self.log.error('基础信息获取失败...{pripid}'.format(pripid=pri_pid))
            return None

        # 解析公司信息
        company = self.get_company_name(base_info.text)
        if company is None:
            self.log.error('获取公司信息失败...{text}'.format(text=base_info.text))
            return None

        # 建立数据模型
        data = self.get_model(company, seed, search_name, self.province)

        # 包含了基本信息,变更信息
        self.append_model(data, Model.base_info, base_info_url, base_info.text)

        # 出资信息
        self.append_model(data, Model.contributive_info, base_info_url, base_info.text)

        # 出资信息详情
        self.get_contributive_info_detail(session, data, base_info.text)

        # 获取股东信息
        url = 'http://{host}/pub/jsxx/{pripid}'.format(
            host=self.host, pripid=pri_pid)
        r = self.task_request(session, session.post, url)
        if r is not None:
            self.append_model(data, Model.shareholder_info, url, r.text)
        else:
            self.append_model(data, Model.shareholder_info, url, '', status=self.STATUS_FAIL)

        # 获取年报信息
        annual_url = 'http://{host}/pub/annual/{flag}/{pripid}'.format(
            host=self.host, pripid=pri_pid, flag=flag)
        annual_info = self.task_request(session, session.post, annual_url)
        if annual_info is not None:
            # self.append_model(data, Model.annual_info, annual_url, annual_info.text, classify=Model.type_list)
            self.get_annual_detail(session, annual_info.text, data)
        # else:
        #     self.append_model(data, Model.annual_info, annual_url, '',
        #                       status=self.STATUS_FAIL,
        #                       classify=Model.type_list)

        return data

    def get_pri_pid(self, session, pid, flag):
        url = 'http://{host}/pub/jbxx/{flag}/{pid}/jbxx'.format(
            host=self.host, pid=pid, flag=flag)

        proxy = self.get_random_proxy()['http']
        text, cookies = self.web_phantomjs(url, session.headers, proxy)
        if text is None:
            self.log.warn("phantomjs 获取网页失败..")
            return None

        # r = self.task_request(session, session.get, url)
        # if r is None:
        #     return None

        pri_pid = PyQuery(text, parser='html').find("#pripid").attr('value')
        self.log.info("pripid = {}".format(pri_pid))

        return pri_pid

    def get_detail_html_list(self, seed, session, detail_list):
        data_list = []
        for item in detail_list:
            try:
                pid = item.get('pripid', None)
                if pid is None:
                    self.log.error('参数错误: item = {item}'.format(item=item))
                    continue

                search_name = item.get('search_name', None)
                if search_name is None:
                    self.log.error('参数错误: item = {item}'.format(item=item))
                    continue

                data = self.get_detail_html_list_qy(seed, search_name, session, pid, 'qy')
                if data is not None:
                    data_list.append(data)
                    continue

                self.log.warn('qy flag 获取企业信息失败 重试 gt flag..pripid = {pid}'.format(
                    pid=pid))

                data = self.get_detail_html_list_qy(seed, search_name, session, pid, 'gt')
                if data is not None:
                    data_list.append(data)

                    self.log.warn('gt flag 获取信息成功.. pripid = {pid}'.format(
                        pid=pid))
                    continue

                self.log.warn('gt flag 获取信息失败.. pripid = {pid}'.format(
                    pid=pid))

            except Exception as e:
                self.log.exception(e)

        return self.sent_to_target(data_list)
